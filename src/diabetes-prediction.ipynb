{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab8da90",
   "metadata": {},
   "source": [
    "# Diabetes Prediction and Analysis\n",
    "\n",
    "In this project, the researchers' aim are to investigate and predict the likelihood of diabetes in individuals by leveraging a publicly available dataset. The researchers' approach involves conducting comprehensive exploratory data analysis, comparing key health metrics between diabetic and non-diabetic groups, and developing machine learning models for prediction. \n",
    "\n",
    "The primary objective is to identify significant health indicators linked to diabetes and to construct reliable models that can support early detection efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53302e",
   "metadata": {},
   "source": [
    "**Dataset used:**\n",
    "\n",
    "    Clinical health records\n",
    "    Key features: gender, age, hypertension, heart_disease, smoking_history, bmi, HbA1c_level, blood_glucose_level\n",
    "    Target Variable: diabetes\n",
    "    \n",
    "\n",
    "**Key Analytics Questions Solved:**\n",
    "\n",
    "    Q1: Which features are most important for predicting diabetes?\n",
    "    Q3: What is the best model for this Diabetes Prediction?\n",
    "\n",
    "\n",
    "**Models used:**\n",
    "\n",
    "    KNN\n",
    "    Logistic Regression (L1 and L2)\n",
    "    SVM (L1 and L2)\n",
    "\n",
    " \n",
    "**Model Results Summary**\n",
    "\n",
    "\n",
    "\n",
    "**Top Predictors:**\n",
    "\n",
    "    \n",
    "\n",
    "**Researchers:**\n",
    "\n",
    "    Maghinay, Shane\n",
    "    Pesaras, Nilmar\n",
    "    Baguio, Ryan\n",
    "    Ventic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d0204",
   "metadata": {},
   "source": [
    "**Definition of Terms**\n",
    "\n",
    "**gender:** refers to the biological sex of the individual, which can have an impact on their susceptibility to diabetes.\n",
    "\n",
    "**age:** an important factor as diabetes is more commonly diagnosed in older adults. Age ranges from 0-80 in our dataset.\n",
    "\n",
    "**hypertension:** medical condition in which the blood pressure in the arteries is persistently elevated. It has values a 0 or 1 where 0 indicates they don’t have hypertension and for 1 it means they have hypertension.\n",
    "\n",
    "**heart_disease:** another medical condition that is associated with an increased risk of developing diabetes. It has values a 0 or 1 where 0 indicates they don’t have heart disease and for 1 it means they have heart disease.\n",
    "\n",
    "**smoking_history:** considered a risk factor for diabetes and can exacerbate the complications associated with diabetes.In our dataset we have 5 categories i.e not current,former,No Info,current,never and ever.\n",
    "\n",
    "**bmi (Body Mass Index):** a measure of body fat based on weight and height. Higher BMI values are linked to a higher risk of diabetes. The range of BMI in the dataset is from 10.16 to 71.55. BMI less than 18.5 is underweight, 18.5-24.9 is normal, 25-29.9 is overweight, and 30 or more is obese. \n",
    "\n",
    "**HbA1c_level (Hemoglobin A1c):** measure of a person's average blood sugar level over the past 2-3 months. Higher levels indicate a greater risk of developing diabetes. Mostly more than 6.5% of HbA1c Level indicates diabetes.\n",
    "\n",
    "**blood_glucose_level:** refers to the amount of glucose in the bloodstream at a given time. High blood glucose levels are a key indicator of diabetes.\n",
    "\n",
    "**diabetes:** target variable being predicted, with values of 1 indicating the presence of diabetes and 0 indicating the absence of diabetes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728eb2e-6db7-470c-8da8-6f18068b39cf",
   "metadata": {},
   "source": [
    "<center> --------------------------------------------------------------------------------- <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28c5b5",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcef687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler # Use for encoding categorical variables\n",
    "from sklearn.model_selection import train_test_split # Use for splitting the dataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score # Use for evaluating the model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4797b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('diabetes_prediction_dataset.csv')\n",
    "\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6eac8",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before doing the preprocessing, the researchers did a data exploration like:\n",
    "1. Finding Missing Values\n",
    "2. Finding Null Values\n",
    "3. Finding Duplicate Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Dataset (Rows, Columns):\", data_set.shape, \"\\n\") # total number of rows and columns\n",
    "print(\"Columns in Dataset:\\n\", data_set.columns, \"\\n\") # names of columns\n",
    "print(\"First 5 Rows:\\n\", data_set.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Missing Values\n",
    "print(\"Missing values per column:\")\n",
    "print(data_set.isnull().sum())\n",
    "\n",
    "# Check for duplicate values\n",
    "print(\"\\nDuplicate values:\" , data_set.duplicated().sum())\n",
    "\n",
    "# Find data types\n",
    "print(\"\\nData types of each column:\")\n",
    "print(data_set.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e0d04",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "\n",
    "**Number of Records:** 100,000\n",
    "**Number of Columns:** 9\n",
    "\n",
    "**Missing Values:**\n",
    "\n",
    "    There are no missing and null values.\n",
    "    \n",
    "**Duplicates:**\n",
    "\n",
    "    Base from the above result, there are `3854` duplicates. This will lead to:\n",
    "    1. Training Bias -> becomes biased toward patterns in frequently repeated observations.\n",
    "    2. Overfitting -> Reduced the ability to perform well on new, unseen data.\n",
    "    3. Distorted Feature Importance -> lead to incorrect conclusions about which health indicators truly predicted diabetes.\n",
    "\n",
    "**Categorical Features:**\n",
    "\n",
    "    gender\n",
    "    smoking_history\n",
    "\n",
    "**Numerical Features:**\n",
    "\n",
    "    age\n",
    "    hypertension\n",
    "    heart_disease\n",
    "    bmi\n",
    "    HbA1c_level\n",
    "    blood_glucose_level\n",
    "    diabetes\n",
    "\n",
    "**Target Variable:**\n",
    "\n",
    "    diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6be5c1",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2afe86",
   "metadata": {},
   "source": [
    "In this section, the researchers will process the data before using it for training. The researchers will do the following:\n",
    "1. Removing duplicates\n",
    "2. Encoding Categorical Data\n",
    "3. Scaling numerical features\n",
    "4. Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c99647",
   "metadata": {},
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a512e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate values\n",
    "\n",
    "data_set.drop_duplicates(inplace=True)\n",
    "\n",
    "# Check for duplicate values again \n",
    "print(\"\\nDuplicate values after removing duplicates:\" , data_set.duplicated().sum())\n",
    "\n",
    "data_set.isnull().sum() # Check for missing values again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8fd5cf",
   "metadata": {},
   "source": [
    "### Encoding Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e769c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = data_set.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Apply One-Hot Encoding for non-binary categorical columns\n",
    "non_binary_columns = [col for col in categorical_columns if data_set[col].nunique() > 2]\n",
    "data_set = pd.get_dummies(data_set, columns=non_binary_columns, drop_first=True)\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "data_set = data_set.astype(int, errors='ignore')\n",
    "\n",
    "print(\"\\nApplied One-Hot Encoding to non-binary categorical columns and converted boolean columns to integers.\\n\")\n",
    "\n",
    "# Display the first few rows of the dataset after encoding\n",
    "print(\"Dataset after encoding categorical variables:\")\n",
    "print(data_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe9c6c",
   "metadata": {},
   "source": [
    "**Jusitifcation For One-Hot Encoding**\n",
    "\n",
    "**Gender Variable (3 categories)**\n",
    "\n",
    "    Categories: Female, Male, Other\n",
    "    Why One-Hot:\n",
    "        * No ordinal relationship exists\n",
    "        * Prevents artificial ordering\n",
    "        * Avoids bias in model interpretation\n",
    "    Resulting Columns:\n",
    "        * gender_Male\n",
    "        * gender_Other\n",
    "        * (Female as reference category)\n",
    "        \n",
    "**Smoking History (6 categories)**\n",
    "    \n",
    "    Categories: never, No Info, current, former, ever, not current\n",
    "    Why One-Hot:\n",
    "        * No inherent order between categories\n",
    "        * Each category has distinct medical significance\n",
    "        * Preserves independence of categories\n",
    "    Resulting Columns:\n",
    "        * smoking_history_current\n",
    "        * smoking_history_ever\n",
    "        * smoking_history_former\n",
    "        * smoking_history_not_current\n",
    "        * smoking_history_No Info\n",
    "        * (never as reference category)\n",
    "        \n",
    "**Benefits of Approach**\n",
    "\n",
    "        Maintains categorical nature of variables\n",
    "        Prevents ordinal assumptions\n",
    "        Allows model to learn category-specific effects\n",
    "        drop_first=True prevents multicollinearity\n",
    "        *Preserves all categorical information without imposing hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636be22",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in data_set.columns if col != \"diabetes\"] # Exclude the target variable\n",
    "\n",
    "scaler = StandardScaler() # StandardScaler for standardization\n",
    "\n",
    "data_set[feature_columns] = scaler.fit_transform(data_set[feature_columns])\n",
    "\n",
    "# Print the first 5 rows of the scaled data\n",
    "print(\"\\nScaled Data:\\n\")\n",
    "print(data_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35fb6b",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_columns = data_set.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = data_set[numerical_columns].corr()\n",
    "\n",
    "# Define your desired column order (must match exactly what's in numerical_columns)\n",
    "desired_order = [\n",
    "    'age', \n",
    "    'hypertension', \n",
    "    'heart_disease', \n",
    "    'bmi', \n",
    "    'HbA1c_level', \n",
    "    'blood_glucose_level',\n",
    "    'diabetes'\n",
    "]\n",
    "\n",
    "# Ensure all columns are accounted for\n",
    "desired_order = [col for col in desired_order if col in numerical_columns]\n",
    "\n",
    "# Reindex the correlation matrix with the desired order\n",
    "corr_matrix = corr_matrix.reindex(index=desired_order, columns=desired_order)\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_cols = data_set[numerical_columns].isnull().sum()\n",
    "missing_cols = missing_cols[missing_cols > 0].index\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap=\"coolwarm\", \n",
    "    center=0,  \n",
    "    linewidths=0.10,  \n",
    "    square=True,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"shrink\": .8, \"label\": \"Correlation Coefficient\"},\n",
    ")\n",
    "\n",
    "# Highlight missing value columns (if any)\n",
    "for col in missing_cols:\n",
    "    if col in desired_order:\n",
    "        col_idx = desired_order.index(col)\n",
    "        ax.add_patch(plt.Rectangle((col_idx, 0), 1, len(desired_order), fill=False, edgecolor='green', lw=4))\n",
    "\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ca162-fe5e-4e9e-afb5-a3a10ed441b3",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f29a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"diabetes\"\n",
    "X = data_set.drop(columns=[target]) # Features\n",
    "y = data_set[target] # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc1755",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5d7d2",
   "metadata": {},
   "source": [
    "In this section, we will train our model based from this order:\n",
    "1. KNN\n",
    "2. Logistic Regression L2\n",
    "3. Logistic Regression L1\n",
    "4. SVM L2\n",
    "5. SVM L1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b73b2",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1adb3",
   "metadata": {},
   "source": [
    "To determine the optimal KNN model for Diabetes prediction, the researchers conducted a comprehensive analysis across multiple parameters and evaluation metrics. The investigation involved testing different numbers of neighbors (k values ranging from 1 to 12, various train-test split ratios (15% to 35%), and multiple random states to ensure robust results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d99b4-91f4-46fb-914f-75332d8c7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ea717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges\n",
    "test_sizes = [0.2, 0.25, 0.3, 0.35]\n",
    "random_states = range(0, 2)\n",
    "neighbors_range = range(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fac14-5866-4af9-8c0f-c9f3d9cab9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c3569-c291-4fa7-878d-add1533cb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "# Step 1: Iterate over different test sizes\n",
    "for test_size in test_sizes:\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    # Step 2: Iterate over random states for variability\n",
    "    for random_state in random_states:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Temporarily use k=5 for initial training\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_train_pred = knn.predict(X_train_scaled)\n",
    "        y_test_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "        train_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "        test_scores.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    avg_train_accuracy = np.mean(train_scores)\n",
    "    avg_test_accuracy = np.mean(test_scores)\n",
    "\n",
    "    # Step 3: Perform CV to find the best k\n",
    "    best_k = 0\n",
    "    best_score = 0\n",
    "    scores = []\n",
    "\n",
    "    for k in neighbors_range:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        cv_scores = cross_val_score(knn, X, y, cv=6, scoring='accuracy')\n",
    "        avg_cv_score = np.mean(cv_scores)\n",
    "        scores.append(avg_cv_score)\n",
    "\n",
    "        if avg_cv_score > best_score:\n",
    "            best_score = avg_cv_score\n",
    "            best_k = k\n",
    "\n",
    "    # Store the results for this test size\n",
    "    results.append((test_size, best_k, avg_train_accuracy, avg_test_accuracy, scores))\n",
    "\n",
    "    # Step 4: Show the best k for each test size (with CV)\n",
    "    print(f\"Test Size: {test_size}, Best k: {best_k}, CV Accuracy: {best_score:.4f}\")\n",
    "\n",
    "    # Step 5: Plot the CV accuracy vs. k for this test size\n",
    "    plt.plot(neighbors_range, scores, label=f\"Test size = {test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c295e94-695e-4f85-ace3-ee113c2d1fa8",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Display the results for all test sizes\n",
    "for test_size, best_k, avg_train, avg_test, _ in results:\n",
    "    print(f\"Test Size: {test_size}, Best k: {best_k}, \"\n",
    "          f\"Avg Train Accuracy: {avg_train * 100:.2f}%, \"\n",
    "          f\"Avg Test Accuracy: {avg_test * 100:.2f}%\")\n",
    "\n",
    "# Step 8: Get the best test size (based on Avg Test Accuracy)\n",
    "best_result = max(results, key=lambda x: x[3])  # x[3] is Avg Test Accuracy\n",
    "\n",
    "best_knn_test_size = best_result[0]\n",
    "best_knn_k = best_result[1]\n",
    "best_knn_train_acc = best_result[2]\n",
    "best_knn_test_acc = best_result[3]\n",
    "\n",
    "print(f\"\\nBest Test Size: {best_knn_test_size}, Best k: {best_knn_k}, \"\n",
    "      f\"Avg Train Accuracy: {best_knn_train_acc * 100:.2f}%, \"\n",
    "      f\"Avg Test Accuracy: {best_knn_test_acc * 100:.2f}%\")\n",
    "\n",
    "# Step 9: Create a DataFrame for the bar plot\n",
    "results_df = pd.DataFrame(results, columns=['Test Size', 'Best k', 'Avg Train Accuracy', 'Avg Test Accuracy', 'CV Scores'])\n",
    "\n",
    "# Melt the DataFrame for plotting\n",
    "results_melted = results_df.melt(id_vars=['Test Size'], \n",
    "                                value_vars=['Avg Train Accuracy', 'Avg Test Accuracy'],\n",
    "                                var_name='Accuracy Type', \n",
    "                                value_name='Accuracy')\n",
    "\n",
    "# Clean up the accuracy type labels\n",
    "results_melted['Accuracy Type'] = results_melted['Accuracy Type'].str.replace('Avg ', '')\n",
    "\n",
    "# Step 10: Plot the bar chart comparing Train and Test Accuracies\n",
    "sns.set_style(\"whitegrid\")\n",
    "palette = {'Train Accuracy': \"#e60f0f\", 'Test Accuracy': \"#e6d816\"}\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.barplot(x='Test Size', y='Accuracy', hue='Accuracy Type', \n",
    "                data=results_melted, palette=palette, dodge=True,\n",
    "                edgecolor='black', linewidth=1, alpha=0.85,\n",
    "                width=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Test Size', fontsize=12, labelpad=10, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12, labelpad=10, fontweight='bold')\n",
    "plt.title(f'Model Performance Across Different Test Sizes', fontsize=14, pad=20, fontweight='bold')\n",
    "\n",
    "# Format y-axis as percentages\n",
    "ax.set_yticklabels(['{:.0f}%'.format(y*100) for y in ax.get_yticks()])\n",
    "\n",
    "# Custom grid settings\n",
    "ax.grid(visible=True, which='major', axis='y', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.grid(visible=True, which='minor', axis='y', linestyle=':', linewidth=0.5, alpha=0.4)\n",
    "ax.minorticks_on()\n",
    "\n",
    "# Improve legend\n",
    "legend = plt.legend(title='Accuracy Type', frameon=True, \n",
    "                   facecolor='white', framealpha=1,\n",
    "                   bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "legend.get_title().set_fontweight('bold')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2., height + 0.01,\n",
    "            '{:.1%}'.format(height),\n",
    "            ha=\"center\", fontsize=9, fontweight='bold')\n",
    "\n",
    "# Adjust layout and appearance\n",
    "sns.despine(left=True, right=True, top=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add horizontal line at 50% for reference\n",
    "plt.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Adjust spacing between bars\n",
    "plt.gca().margins(x=0.1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258465a-0fbe-4989-9cf5-1eae7f360af7",
   "metadata": {},
   "source": [
    "#### Training Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490320e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14f79098",
   "metadata": {},
   "source": [
    "### <center>Logistic Regression<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2382a",
   "metadata": {},
   "source": [
    "#### Logistic Regression L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e567e86-c835-4d39-9bb2-20780f5219e1",
   "metadata": {},
   "source": [
    "##### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for tuning\n",
    "test_sizes = [0.2, 0.25, 0.3, 0.35]\n",
    "random_states = range(0, 2)  # 31 random states (0-50)\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Step 1: Evaluate different test sizes and C values to find the best configuration\n",
    "for test_size in test_sizes:\n",
    "    print(f\"\\nEvaluating Test Size: {test_size}\")\n",
    "    \n",
    "    # Collect (C, train_acc, test_acc, gap) for this test_size\n",
    "    info = []\n",
    "    \n",
    "    for C in C_values:\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        \n",
    "        for rs in random_states:\n",
    "            # Split the data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=rs\n",
    "            )\n",
    "            # Scale the data\n",
    "            scaler = StandardScaler()\n",
    "            X_tr = scaler.fit_transform(X_train)\n",
    "            X_te = scaler.transform(X_test)\n",
    "            \n",
    "            # Train the model\n",
    "            model = LogisticRegression(penalty='l2', C=C, max_iter=10000)\n",
    "            model.fit(X_tr, y_train)\n",
    "            \n",
    "            # Predict and calculate accuracy\n",
    "            train_scores.append(accuracy_score(y_train, model.predict(X_tr)))\n",
    "            test_scores.append(accuracy_score(y_test, model.predict(X_te)))\n",
    "        \n",
    "        # Average scores and gap calculation\n",
    "        avg_train = np.mean(train_scores)\n",
    "        avg_test = np.mean(test_scores)\n",
    "        gap = abs(avg_train - avg_test)\n",
    "        \n",
    "        print(f\"  C = {C:<6} Avg Train Acc = {avg_train*100:6.2f}%, \"\n",
    "              f\"Avg Test Acc = {avg_test*100:6.2f}%, Gap = {gap*100:5.2f}%\")\n",
    "        \n",
    "        info.append((C, avg_train, avg_test, gap))\n",
    "    \n",
    "    # Pick the C with the smallest gap\n",
    "    best_C, best_tr, best_te, best_gap = min(info, key=lambda x: x[3])\n",
    "    results.append((test_size, best_C, best_tr, best_te, best_gap))\n",
    "\n",
    "\n",
    "# Step 3: Compute feature importance for the best test size and best C\n",
    "log_l2_best_test_size = results[-1][0]  # Best test size\n",
    "log_l2_best_C = results[-1][1]  # Best C\n",
    "\n",
    "# Initialize lists to store the accuracies for averaging\n",
    "train_accs_final = []\n",
    "test_accs_final = []\n",
    "\n",
    "# Step 4: Retrain model on the best test size and best C, AVERAGE over random states 0-51\n",
    "for random_state in random_states:\n",
    "    # Split data for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=log_l2_best_test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train the model on the full training set and evaluate on the test set\n",
    "    best_model = LogisticRegression(penalty='l2', C=log_l2_best_C, max_iter=10000)\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    train_acc = best_model.score(X_train_scaled, y_train)\n",
    "    test_acc = best_model.score(X_test_scaled, y_test)\n",
    "\n",
    "    # Store the accuracies to average later\n",
    "    train_accs_final.append(train_acc)\n",
    "    test_accs_final.append(test_acc)\n",
    "\n",
    "# Average the train and test accuracies over all random states\n",
    "log_l2_best_train_accuracy = np.mean(train_accs_final)\n",
    "log_l2_best_test_accuracy = np.mean(test_accs_final)\n",
    "\n",
    "# Feature importance: Calculate absolute values of the coefficients as feature importance\n",
    "log_l2_feature_importance = np.abs(best_model.coef_[0])\n",
    "log_l2_feature_importance_percent = 100 * log_l2_feature_importance / log_l2_feature_importance.sum()\n",
    "\n",
    "# Rank features by importance (descending order)\n",
    "log_l2_ranked_features = np.argsort(log_l2_feature_importance_percent)[::-1]\n",
    "\n",
    "# Get column names (assuming X is your feature DataFrame)\n",
    "log_l2_feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a2577-a415-4473-bc7e-73c0408b04d6",
   "metadata": {},
   "source": [
    "##### Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Print the summary for best C for each test size\n",
    "print(\"\\nSummary of Best C for Each Test Size (L2, Least Overfitting):\")\n",
    "for result in results:\n",
    "    print(f\"Test Size: {result[0]:<5}, Best C: {result[1]:<6}, Train: {result[2]*100:6.2f}%, \"\n",
    "          f\"Test: {result[3]*100:6.2f}%, Gap: {result[4]*100:5.2f}%\")\n",
    "\n",
    "# Summary of Best C and Test Size\n",
    "print(f\"\\nLog L2 Summary:\")\n",
    "print(f\"Best Test Size: {log_l2_best_test_size}\")\n",
    "print(f\"Best C: {log_l2_best_C}\")\n",
    "print(f\"Final Average Train Accuracy: {log_l2_best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Final Average Test Accuracy: {log_l2_best_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Plotting accuracy and gap\n",
    "summary_df = pd.DataFrame(results, columns=[\"Test Size\", \"Best C\", \"Train Accuracy\", \"Test Accuracy\", \"Gap\"])\n",
    "\n",
    "# Melt for Seaborn\n",
    "melted = summary_df.melt(id_vars=[\"Test Size\"], \n",
    "                         value_vars=[\"Train Accuracy\", \"Test Accuracy\", \"Gap\"],\n",
    "                         var_name=\"Metric\", value_name=\"Accuracy\")\n",
    "\n",
    "# Softer color palette for L2 plot\n",
    "color_palette = {\n",
    "    \"Train Accuracy\": \"#0825cc\",\n",
    "    \"Test Accuracy\": \"#0dc4c4\",  # softer red\n",
    "    \"Gap\": \"#db6809\"\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=melted, x=\"Test Size\", y=\"Accuracy\", hue=\"Metric\", \n",
    "                 palette=color_palette, dodge=True)\n",
    "\n",
    "# Make bars thin\n",
    "for container in ax.containers:\n",
    "    for bar in container:\n",
    "        bar.set_width(0.25)\n",
    "\n",
    "# Add percentage labels on top\n",
    "for container in ax.containers:\n",
    "    for bar in container:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height * 100:.1f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 4),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Final styling\n",
    "plt.title(\"Train, Test Accuracy, and Gap per Test Size (L2 Regularization)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Test Size\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce38a11",
   "metadata": {},
   "source": [
    "**Evaluation of L2 Regularization with Varying Test Sizes**\n",
    "\n",
    "This investigates the performance of **L2-regularized logistic regression** across different **test set sizes**, aiming to:\n",
    "\n",
    "- Identify the **optimal test size and regularization strength (C)**.\n",
    "- Minimize the **overfitting gap** between training and testing performance.\n",
    "- Visualize the effects of test size on model generalization.\n",
    "\n",
    "**Results Summary**\n",
    "\n",
    "| Test Size | Best C | Train Accuracy | Test Accuracy | Overfitting Gap |\n",
    "|-----------|--------|----------------|---------------|------------------|\n",
    "| 0.20      | 0.01   | 94.00%         | 94.00%        | **0.00%**        |\n",
    "| 0.25      | 0.10   | 98.38%         | 98.33%        | 0.05%            |\n",
    "| 0.30      | 0.10   | 98.39%         | 98.16%        | 0.23%            |\n",
    "| 0.35      | 0.01   | 93.03%         | 92.92%        | 0.11%            |\n",
    "\n",
    "- **Best Performance** achieved at:\n",
    "  - **Test Size = 0.20**\n",
    "  - **Best C = 0.01**\n",
    "  - **Train Acc = 94.00%**, **Test Acc = 94.00%**\n",
    "\n",
    "## Visualization: Train, Test Accuracy, and Overfitting Gap\n",
    "\n",
    "The bar chart above displays **Train Accuracy**, **Test Accuracy**, and the **Overfitting Gap** (difference between the two) across each test size:\n",
    "\n",
    "**[L2 Regularization Results]**\n",
    "\n",
    "**Key Insights from the Chart:**\n",
    "\n",
    "-  **Train Accuracy (blue)** and  **Test Accuracy (red)** remain close across all test sizes, indicating **strong generalization**.\n",
    "-  **Gap (orange)** is minimal, especially at **0.20** and **0.25**, implying **low overfitting**.\n",
    "- Slight gap increase at **Test Size = 0.30** suggests **mild overfitting** even with good accuracy.\n",
    "- **Lowest train/test accuracy** appears at **0.35**, showing performance drop due to **less training data**.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "- The **optimal test/train split** for this dataset using L2 regularization is:\n",
    "  - **Test Size: 0.20**\n",
    "  - **Regularization Strength (C): 0.01**\n",
    "- This configuration yields the **highest generalization** with **zero overfitting**.\n",
    "- As the test size increases, accuracy slightly drops, and the gap begins to widen, confirming the trade-off between **training data quantity** and **model generalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b2b6b",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a horizontal bar chart for feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create sorted data for plotting (descending order)\n",
    "sorted_idx = log_l2_ranked_features\n",
    "sorted_features = [log_l2_feature_names[i] for i in sorted_idx]\n",
    "sorted_importance = log_l2_feature_importance_percent[sorted_idx]\n",
    "\n",
    "# Reverse the order so highest appears at the top of the plot\n",
    "sorted_features = sorted_features[::-1]\n",
    "sorted_importance = sorted_importance[::-1]\n",
    "\n",
    "# Plot horizontal bars\n",
    "bars = plt.barh(range(len(sorted_features)), sorted_importance, align='center', color='#4287f5')\n",
    "plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "\n",
    "# Add percentage labels to the bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.5, \n",
    "             bar.get_y() + bar.get_height()/2, \n",
    "             f'{sorted_importance[i]:.2f}%', \n",
    "             ha='left', \n",
    "             va='center',\n",
    "             fontweight='bold')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Logistic Regression L2 Feature Importance', fontsize=15)\n",
    "plt.xlabel('Importance (%)', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the ranked feature importances (still in original descending order)\n",
    "print(\"\\nLog L2 Feature Importance (sorted by importance):\")\n",
    "for i, idx in enumerate(log_l2_ranked_features):\n",
    "    print(f\"  Rank {i + 1}: {log_l2_feature_names[idx]} - {log_l2_feature_importance_percent[idx]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a0e8e6",
   "metadata": {},
   "source": [
    "**Interpretations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eacfb",
   "metadata": {},
   "source": [
    "### Logistics Regression L1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab32cc-b527-4b46-aef6-fee43e072ab3",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "test_sizes = [0.2, 0.25, 0.3, 0.35]\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "random_states = range(0, 2) # Changed to 31 for more iterations\n",
    "\n",
    "logistic_l1_results = []  # To store best results per test size\n",
    "\n",
    "# Variables with unique names\n",
    "Log_l1_best_test_size = None\n",
    "Log_l1_best_C = None\n",
    "Log_l1_best_train_acc = 0\n",
    "Log_l1_best_test_acc = 0\n",
    "Log_l1_best_gap = float('inf')\n",
    "\n",
    "# Step 1: Evaluate all combinations of test size and C to find the best configuration\n",
    "for test_size in test_sizes:\n",
    "    print(f\"\\nEvaluating Test Size: {test_size}\")\n",
    "    \n",
    "    c_train_scores = {C: [] for C in C_values}\n",
    "    c_test_scores = {C: [] for C in C_values}\n",
    "    \n",
    "    for random_state in random_states:\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train and evaluate for each C\n",
    "        for C in C_values:\n",
    "            model = LogisticRegression(C=C, penalty='l1', solver='liblinear', max_iter=1000)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "            test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "            c_train_scores[C].append(train_acc)\n",
    "            c_test_scores[C].append(test_acc)\n",
    "    \n",
    "    # Evaluate overfitting and print details\n",
    "    overfit_info = []\n",
    "    for C in C_values:\n",
    "        avg_train = np.mean(c_train_scores[C])\n",
    "        avg_test = np.mean(c_test_scores[C])\n",
    "        gap = abs(avg_train - avg_test)\n",
    "        overfit_info.append((C, avg_train, avg_test, gap))\n",
    "        print(f\"  C = {C}: Avg Train Acc = {avg_train * 100:.2f}%, \"\n",
    "              f\"Avg Test Acc = {avg_test * 100:.2f}%, Gap = {gap * 100:.2f}%\")\n",
    "    \n",
    "    # Pick C with least overfitting (smallest gap)\n",
    "    best_C_for_test_size, best_train, best_test, best_gap_for_test_size = min(overfit_info, key=lambda x: x[3])\n",
    "    \n",
    "    # Track the overall best configuration\n",
    "    if best_gap_for_test_size < Log_l1_best_gap:\n",
    "        Log_l1_best_test_size = test_size\n",
    "        Log_l1_best_C = best_C_for_test_size\n",
    "        Log_l1_best_train_acc = best_train\n",
    "        Log_l1_best_test_acc = best_test\n",
    "        Log_l1_best_gap = best_gap_for_test_size\n",
    "\n",
    "    logistic_l1_results.append((test_size, best_C_for_test_size, best_train, best_test, best_gap_for_test_size))\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Retrain model on the best test size and best C, AVERAGE over random states 0-51\n",
    "train_accs_final = []\n",
    "test_accs_final = []\n",
    "\n",
    "for random_state in random_states:\n",
    "    # Split data for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=Log_l1_best_test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train the model on the full training set and evaluate on the test set\n",
    "    best_model = LogisticRegression(C=Log_l1_best_C, penalty='l1', solver='liblinear', max_iter=1000)\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    train_acc = best_model.score(X_train_scaled, y_train)\n",
    "    test_acc = best_model.score(X_test_scaled, y_test)\n",
    "\n",
    "    # Store the accuracies to average later\n",
    "    train_accs_final.append(train_acc)\n",
    "    test_accs_final.append(test_acc)\n",
    "\n",
    "# Average the train and test accuracies over all random states\n",
    "Log_l1_avg_train_acc_final = np.mean(train_accs_final)\n",
    "Log_l1_avg_test_acc_final = np.mean(test_accs_final)\n",
    "\n",
    "# Get the absolute values of coefficients as feature importance for the final model\n",
    "Log_l1_feature_importances = np.abs(best_model.coef_[0])\n",
    "\n",
    "# Calculate percentage importance for each feature\n",
    "Log_l1_feature_importance_percent = 100 * Log_l1_feature_importances / Log_l1_feature_importances.sum()\n",
    "\n",
    "# Rank features by importance\n",
    "Log_l1_ranked_features = np.argsort(Log_l1_feature_importance_percent)[::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac948c-b604-4dec-9885-dc1cc1674fd4",
   "metadata": {},
   "source": [
    "#### Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99842370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Print the summary for best C for each test size\n",
    "print(\"\\nSummary of Best C for Each Test Size (L1, Least Overfitting):\")\n",
    "for result in logistic_l1_results:\n",
    "    print(f\"Test Size: {result[0]:<5}, Best C: {result[1]:<6}, Train: {result[2]*100:6.2f}%, \"\n",
    "          f\"Test: {result[3]*100:6.2f}%, Gap: {result[4]*100:5.2f}%\")\n",
    "# Step 4: Output feature importance\n",
    "print(f\"\\nBest Test Size: {Log_l1_best_test_size}, Best C: {Log_l1_best_C}\")\n",
    "\n",
    "# Print out the averaged accuracies\n",
    "print(f\"\\nFinal Average Train Accuracy (over random states 0-51): {Log_l1_avg_train_acc_final * 100:.2f}%\")\n",
    "print(f\"Final Average Test Accuracy (over random states 0-51): {Log_l1_avg_test_acc_final * 100:.2f}%\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "summary_df = pd.DataFrame(logistic_l1_results, columns=[\"Test Size\", \"Best C\", \"Train Accuracy\", \"Test Accuracy\", \"Gap\"])\n",
    "\n",
    "# Melt for Seaborn\n",
    "melted = summary_df.melt(id_vars=[\"Test Size\"], \n",
    "                         value_vars=[\"Train Accuracy\", \"Test Accuracy\", \"Gap\"],\n",
    "                         var_name=\"Metric\", value_name=\"Accuracy\")\n",
    "\n",
    "# Softer color palette for L1 plot\n",
    "color_palette = {\n",
    "    \"Train Accuracy\": \"#0825cc\",\n",
    "    \"Test Accuracy\": \"#0dc4c4\",  # softer red\n",
    "    \"Gap\": \"#db6809\"\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=melted, x=\"Test Size\", y=\"Accuracy\", hue=\"Metric\", \n",
    "                 palette=color_palette, dodge=True)\n",
    "\n",
    "# Make bars thin\n",
    "for container in ax.containers:\n",
    "    for bar in container:\n",
    "        bar.set_width(0.25)\n",
    "\n",
    "# Add percentage labels on top\n",
    "for container in ax.containers:\n",
    "    for bar in container:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height * 100:.1f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 4),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Final styling\n",
    "plt.title(\"Train, Test Accuracy, and Gap per Test Size (L1 Regularization)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Test Size\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da1461-fd14-416f-958e-ad34467d5241",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229c06a",
   "metadata": {},
   "source": [
    "\n",
    "**L1 Logistic Regression: Train vs Test Accuracy Across Test Sizes**\n",
    "\n",
    "This section analyzes how the model performs using **L1 regularization** across different **test sizes**. It identifies the best `C` value (inverse of regularization strength) that causes the **least overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary of Results (Best `C`, Accuracy & Overfitting Gap)\n",
    "\n",
    "| Test Size | Best C | Train Accuracy | Test Accuracy | Overfitting Gap |\n",
    "|-----------|--------|----------------|----------------|------------------|\n",
    "| 0.20      | 0.01   | 93.80%         | 93.82%         | 0.01%            |\n",
    "| 0.25      | 0.01   | 93.71%         | 93.69%         | 0.02%            |\n",
    "| 0.30      | 0.01   | 93.55%         | 93.51%         | 0.04%            |\n",
    "| 0.35      | 1      | 99.50%         | 99.25%         | 0.24%            |\n",
    "\n",
    " **Best Test Size**: **0.20**  \n",
    " **Best C**: **0.01** (lowest overfitting)\n",
    "\n",
    "---\n",
    "\n",
    "###  Chart Explanation\n",
    "\n",
    "The bar chart above compares:\n",
    "\n",
    "- **Train Accuracy** (how well the model fits the training data),\n",
    "- **Test Accuracy** (how well the model generalizes to new data), and\n",
    "- **Gap** (the difference between Train and Test Accuracy – smaller is better).\n",
    "\n",
    "It helps visualize overfitting:  \n",
    "> **Larger gaps mean more overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis base from the Bar Plots**\n",
    "\n",
    "- **Test size 0.20** performs best overall with both train and test accuracy at **~93.8%** and **almost zero gap**.\n",
    "- At **test size 0.35**, train accuracy is very high (**99.5%**) but with a larger gap (**0.24%**), showing signs of **overfitting**.\n",
    "- The model is **most stable** with smaller test sizes and lower `C` values.\n",
    "\n",
    "---\n",
    "\n",
    "**Key insights**\n",
    "\n",
    "This analysis helps in:\n",
    "- Choosing the **best model configuration** (test size + C value),\n",
    "- Ensuring the model **generalizes well** (avoids overfitting),\n",
    "- Identifying the **sweet spot** for data splitting.\n",
    "\n",
    ">  Use **Test Size = 0.20** and **C = 0.01** for the best balance between performance and stability.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeee01",
   "metadata": {},
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49828d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a horizontal bar chart for feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Define feature names first (this was missing)\n",
    "Log_l1_feature_names = X.columns.tolist()\n",
    "\n",
    "# Create sorted data for plotting (descending order)\n",
    "sorted_idx = Log_l1_ranked_features\n",
    "sorted_features = [Log_l1_feature_names[i] for i in sorted_idx]\n",
    "sorted_importance = Log_l1_feature_importance_percent[sorted_idx]  # Changed from log_l2_ to Log_l1_\n",
    "\n",
    "# Reverse the order so highest appears at the top of the plot\n",
    "sorted_features = sorted_features[::-1]\n",
    "sorted_importance = sorted_importance[::-1]\n",
    "\n",
    "# Plot horizontal bars\n",
    "bars = plt.barh(range(len(sorted_features)), sorted_importance, align='center', color='#4287f5')\n",
    "plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "\n",
    "# Add percentage labels to the bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.5, \n",
    "             bar.get_y() + bar.get_height()/2, \n",
    "             f'{sorted_importance[i]:.2f}%', \n",
    "             ha='left', \n",
    "             va='center',\n",
    "             fontweight='bold')\n",
    "\n",
    "# Add title and labels - changed to L1 instead of L2\n",
    "plt.title('Logistic Regression L1 Feature Importance', fontsize=15)\n",
    "plt.xlabel('Importance (%)', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the ranked feature importances (now using L1 variables)\n",
    "print(\"\\nLog L1 Feature Importance (sorted by importance):\")\n",
    "for i, idx in enumerate(Log_l1_ranked_features):\n",
    "    print(f\"  Rank {i + 1}: {Log_l1_feature_names[idx]} - {Log_l1_feature_importance_percent[idx]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac30ea5",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
